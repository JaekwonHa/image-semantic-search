{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned-iridium-385115\n",
      "whjang-titanic\n",
      "preprocess\n",
      "train\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/rawdata/train.csv\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/preprocdata/processed_train.csv\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/latestacc/accuracy.csv\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/model/model.pkl\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/stage\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/train/titanic_train.tar.gz\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/train/output/\n"
     ]
    }
   ],
   "source": [
    "#titanic_kfp_pipeline.ipynb\n",
    "#Copyright 2020 Google LLC. \n",
    "#This software is provided as-is, without warranty or representation for any use or purpose. \n",
    "#Your use of it is subject to your agreements with Google.\n",
    "#Author: whjang@google.com\n",
    "\n",
    "PROJECT_ID = \"learned-iridium-385115\"\n",
    "IMAGE_PREFIX = \"whjang-titanic\"\n",
    "PREPROC_DIR = \"preprocess\"\n",
    "TRAIN_DIR = \"train\"\n",
    "\n",
    "WORK_BUCKET = \"gs://practice-tensorflow/3.tutorial-kubeflow\"\n",
    "RAW_CSV_GCS_URI = WORK_BUCKET + \"/rawdata/train.csv\"\n",
    "PREPROC_CSV_GCS_URI = WORK_BUCKET + \"/preprocdata/processed_train.csv\"\n",
    "ACC_CSV_GCS_URI = WORK_BUCKET + \"/latestacc/accuracy.csv\"\n",
    "MODEL_PKL_GCS_URI = WORK_BUCKET + \"/model/model.pkl\"\n",
    "STAGE_GCS_FOLDER = WORK_BUCKET + \"/stage\"\n",
    "\n",
    "AIPJOB_TRAINER_GCS_PATH = WORK_BUCKET + \"/train/titanic_train.tar.gz\"\n",
    "AIPJOB_OUTPUT_GCS_PATH = WORK_BUCKET + \"/train/output/\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"IMAGE_PREFIX\"] = IMAGE_PREFIX\n",
    "os.environ[\"PREPROC_DIR\"] = PREPROC_DIR\n",
    "os.environ[\"TRAIN_DIR\"] = TRAIN_DIR\n",
    "\n",
    "os.environ[\"WORK_BUCKET\"] = WORK_BUCKET\n",
    "os.environ[\"RAW_CSV_GCS_URI\"] = RAW_CSV_GCS_URI\n",
    "os.environ[\"PREPROC_CSV_GCS_URI\"] = PREPROC_CSV_GCS_URI\n",
    "os.environ[\"ACC_CSV_GCS_URI\"] = ACC_CSV_GCS_URI\n",
    "os.environ[\"MODEL_PKL_GCS_URI\"] = MODEL_PKL_GCS_URI\n",
    "os.environ[\"STAGE_GCS_FOLDER\"] = STAGE_GCS_FOLDER\n",
    "os.environ[\"AIPJOB_TRAINER_GCS_PATH\"] = AIPJOB_TRAINER_GCS_PATH\n",
    "os.environ[\"AIPJOB_OUTPUT_GCS_PATH\"] = AIPJOB_OUTPUT_GCS_PATH\n",
    "\n",
    "!echo $PROJECT_ID\n",
    "!echo $IMAGE_PREFIX\n",
    "!echo $PREPROC_DIR\n",
    "!echo $TRAIN_DIR\n",
    "!echo $WORK_BUCKET\n",
    "!echo $RAW_CSV_GCS_URI\n",
    "!echo $PREPROC_CSV_GCS_URI\n",
    "!echo $ACC_CSV_GCS_URI\n",
    "!echo $MODEL_PKL_GCS_URI\n",
    "!echo $STAGE_GCS_FOLDER\n",
    "!echo $AIPJOB_TRAINER_GCS_PATH\n",
    "!echo $AIPJOB_OUTPUT_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/10 : FROM amancevice/pandas\n",
      " ---> 913743377700\n",
      "Step 2/10 : RUN pip3 install --upgrade pandas\n",
      " ---> Using cache\n",
      " ---> c50a88bd7679\n",
      "Step 3/10 : RUN pip3 install --upgrade gcsfs\n",
      " ---> Using cache\n",
      " ---> e3937fb8d6ce\n",
      "Step 4/10 : RUN pip3 install --upgrade argparse\n",
      " ---> Using cache\n",
      " ---> ca55437f722b\n",
      "Step 5/10 : RUN pip3 install sklearn\n",
      " ---> Using cache\n",
      " ---> e1bca2a336e5\n",
      "Step 6/10 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> 839afde6bcb5\n",
      "Step 7/10 : RUN mkdir -p /titanic/src\n",
      " ---> Using cache\n",
      " ---> cd2b35fb86b5\n",
      "Step 8/10 : COPY . /titanic/src\n",
      " ---> Using cache\n",
      " ---> 15dfff23d5c0\n",
      "Step 9/10 : WORKDIR /titanic/src\n",
      " ---> Using cache\n",
      " ---> f4ab1b99ef4d\n",
      "Step 10/10 : ENTRYPOINT [\"python\", \"titanic_preprocess.py\"]\n",
      " ---> Using cache\n",
      " ---> d364eb9577a0\n",
      "Successfully built d364eb9577a0\n",
      "Successfully tagged whjang-titanic-preprocess:latest\n",
      "The push refers to repository [gcr.io/learned-iridium-385115/whjang-titanic-preprocess]\n",
      "\n",
      "\u001B[1Bf8663698: Preparing \n",
      "\u001B[1B4c1dfa60: Preparing \n",
      "\u001B[1Bdb8c4555: Preparing \n",
      "\u001B[1Bb0765994: Preparing \n",
      "\u001B[1B9274cdab: Preparing \n",
      "\u001B[1B16fbc8c8: Preparing \n",
      "\u001B[1Bf9850ac6: Preparing \n",
      "\u001B[1B4e579163: Preparing \n",
      "\u001B[1B338d328a: Preparing \n",
      "\u001B[1B857d7bd5: Preparing \n",
      "\u001B[1B32a549a2: Preparing \n",
      "\u001B[1B8e3386b6: Preparing \n",
      "\u001B[1B5e673844: Preparing \n",
      "\u001B[1B811d364e: Preparing \n",
      "\u001B[1B260e173a: Preparing \n",
      "\u001B[1Bbb98b0dc: Preparing \n",
      "\u001B[1B8387f467: Preparing \n",
      "\u001B[12B9850ac6: Pushed   193.5MB/188.3MB\u001B[18A\u001B[2K\u001B[15A\u001B[2K\u001B[14A\u001B[2K\u001B[16A\u001B[2K\u001B[16A\u001B[2K\u001B[14A\u001B[2K\u001B[16A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[14A\u001B[2K\u001B[18A\u001B[2K\u001B[13A\u001B[2K\u001B[13A\u001B[2K\u001B[15A\u001B[2K\u001B[17A\u001B[2K\u001B[12A\u001B[2K\u001B[11A\u001B[2K\u001B[11A\u001B[2K\u001B[16A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[14A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[9A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[13A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[8A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[7A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[6A\u001B[2K\u001B[10A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[5A\u001B[2K\u001B[12A\u001B[2K\u001B[4A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[3A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[1A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2K\u001B[12A\u001B[2Klatest: digest: sha256:ef2d9d38efd2a0fdb42f883813524d621df481f74c50076a710ffdc19f99305b size: 4104\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$PREPROC_DIR $PREPROC_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$PREPROC_DIR:latest gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$PREPROC_DIR:latest\n",
    "!docker push gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$PREPROC_DIR:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                               TAG       IMAGE ID       CREATED         SIZE\n",
      "whjang-titanic-preprocess                latest    d364eb9577a0   2 minutes ago   1.14GB\n",
      "amancevice/pandas                        latest    913743377700   9 days ago      1.11GB\n",
      "frolvlad/alpine-python-machinelearning   latest    edb47bd01cc6   2 months ago    399MB\n"
     ]
    }
   ],
   "source": [
    "!docker image ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Titanic Data\n",
      "   Pclass  SibSp  Sex_female  Sex_male  Survived\n",
      "0       3      1       False      True         0\n",
      "1       1      1        True     False         1\n",
      "2       3      0        True     False         1\n",
      "3       1      1        True     False         1\n",
      "4       3      0       False      True         0\n"
     ]
    }
   ],
   "source": [
    "!docker run gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$PREPROC_DIR:latest --raw_csv_gcs_uri $RAW_CSV_GCS_URI --preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 train, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practice-tensorflow/3.tutorial-kubeflow/preprocdata/processed_train.csv\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/model/model.pkl\n",
      "gs://practice-tensorflow/3.tutorial-kubeflow/latestacc/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "!echo $PREPROC_CSV_GCS_URI\n",
    "!echo $MODEL_PKL_GCS_URI\n",
    "!echo $ACC_CSV_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  14.85kB\n",
      "Step 1/12 : FROM amancevice/pandas\n",
      " ---> 913743377700\n",
      "Step 2/12 : RUN pip3 install --upgrade google-cloud-storage\n",
      " ---> Using cache\n",
      " ---> 879ce7aae86c\n",
      "Step 3/12 : RUN pip3 install --upgrade gcsfs\n",
      " ---> Using cache\n",
      " ---> d7104851174f\n",
      "Step 4/12 : RUN pip3 install --upgrade scikit-learn\n",
      " ---> Using cache\n",
      " ---> 43744646c9db\n",
      "Step 5/12 : RUN pip3 install --upgrade argparse\n",
      " ---> Using cache\n",
      " ---> cdbbfbc9b3fe\n",
      "Step 6/12 : RUN pip3 install --upgrade pandas\n",
      " ---> Using cache\n",
      " ---> a1a30c2227ca\n",
      "Step 7/12 : RUN pip3 install --upgrade simplejson\n",
      " ---> Using cache\n",
      " ---> 2b6d9307faa5\n",
      "Step 8/12 : ENV PYTHONUNBUFFERED 1\n",
      " ---> Using cache\n",
      " ---> 46c41aab76d1\n",
      "Step 9/12 : RUN mkdir -p /titanic/src\n",
      " ---> Using cache\n",
      " ---> 21d3e2614cc7\n",
      "Step 10/12 : COPY . /titanic/src\n",
      " ---> 71d9c3dff8fa\n",
      "Step 11/12 : WORKDIR /titanic/src\n",
      " ---> Running in 7ff7cfafb1b9\n",
      "Removing intermediate container 7ff7cfafb1b9\n",
      " ---> b24bfc41e757\n",
      "Step 12/12 : ENTRYPOINT [\"python\", \"titanic_train.py\"]\n",
      " ---> Running in 7cf1a0b5cda0\n",
      "Removing intermediate container 7cf1a0b5cda0\n",
      " ---> f3b68088af56\n",
      "Successfully built f3b68088af56\n",
      "Successfully tagged whjang-titanic-train:latest\n",
      "The push refers to repository [gcr.io/learned-iridium-385115/whjang-titanic-train]\n",
      "\n",
      "\u001B[1B987688b3: Preparing \n",
      "\u001B[1Be434c62b: Preparing \n",
      "\u001B[1B8bdf1d7c: Preparing \n",
      "\u001B[1B9eb936a1: Preparing \n",
      "\u001B[1Bb56f23ac: Preparing \n",
      "\u001B[1Bb1c34c83: Preparing \n",
      "\u001B[1Bfa01cd56: Preparing \n",
      "\u001B[1B0d68e89e: Preparing \n",
      "\u001B[1Bf9850ac6: Preparing \n",
      "\u001B[1B4e579163: Preparing \n",
      "\u001B[1B338d328a: Preparing \n",
      "\u001B[1B857d7bd5: Preparing \n",
      "\u001B[1B32a549a2: Preparing \n",
      "\u001B[1B8e3386b6: Preparing \n",
      "\u001B[1B5e673844: Preparing \n",
      "\u001B[1B811d364e: Preparing \n",
      "\u001B[1B260e173a: Preparing \n",
      "\u001B[1Bbb98b0dc: Preparing \n",
      "\u001B[1B8387f467: Preparing \n",
      "\u001B[20B87688b3: Pushed lready exists 3kB\u001B[20A\u001B[2K\u001B[19A\u001B[2K\u001B[15A\u001B[2K\u001B[11A\u001B[2K\u001B[9A\u001B[2K\u001B[7A\u001B[2K\u001B[3A\u001B[2K\u001B[2A\u001B[2K\u001B[20A\u001B[2Klatest: digest: sha256:68adb4dff053864dd5cf6eb02953b3b00b295eb3901a4fbda919735b87b5b6ef size: 4525\n",
      "train titanic model\n",
      "RF Model Score:  0.8162692847124825\n",
      "evaluation model\n",
      "No accuracy file, we will create one\n",
      "confusion matrix\n",
      "[[92 15]\n",
      " [30 41]]\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80       107\n",
      "           1       0.73      0.58      0.65        71\n",
      "\n",
      "    accuracy                           0.75       178\n",
      "   macro avg       0.74      0.72      0.72       178\n",
      "weighted avg       0.75      0.75      0.74       178\n",
      "\n",
      "\n",
      "accuracy score\n",
      "0.7471910112359551\n",
      "\bWriting metrics file: /mlpipeline-metrics.json\n",
      "latestacc: 0.0\n",
      "min_acc_progress: 1e-06\n",
      "File model.pkl uploaded\n",
      "Write to GCS:gs://practice-tensorflow/3.tutorial-kubeflow/latestacc/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_PREFIX-$TRAIN_DIR $TRAIN_DIR/.\n",
    "!docker tag $IMAGE_PREFIX-$TRAIN_DIR:latest gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker push gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$TRAIN_DIR:latest\n",
    "!docker run gcr.io/$PROJECT_ID/$IMAGE_PREFIX-$TRAIN_DIR:latest \\\n",
    "--preproc_csv_gcs_uri $PREPROC_CSV_GCS_URI \\\n",
    "--model_pkl_gcs_uri $MODEL_PKL_GCS_URI \\\n",
    "--acc_csv_gcs_uri $ACC_CSV_GCS_URI \\\n",
    "--min_acc_progress 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pipeline setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -fr titanic_train.tar.gz\n",
    "!tar zcvf titanic_train.tar.gz *\n",
    "!gsutil cp titanic_train.tar.gz $AIPJOB_TRAINER_GCS_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Kubeflow Pipeline 구성 코드 작성, AI Platform Pipeline 등록 및 실행"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_toolbelt.adapters.appengine'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#titanic_kfp_pipeline.ipynb\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Copyright 2020 Google LLC.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#This software is provided as-is, without warranty or representation for any use or purpose.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#Your use of it is subject to your agreements with Google.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Author: whjang@google.com\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#!pip3 install -U kfp\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcomp\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dsl\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/mlops/lib/python3.11/site-packages/kfp/__init__.py:24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m containers\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dsl\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Client\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_runners\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/mlops/lib/python3.11/site-packages/kfp/_client.py:34\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompiler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compiler\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompiler\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_k8s_helper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sanitize_k8s_name\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkfp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_auth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_auth_token, get_gcp_access_token\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# TTL of the access token associated with the client. This is needed because\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# `gcloud auth print-access-token` generates a token with TTL=1 hour, after\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# which the authentication expires. This TTL is needed for kfp.Client()\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# initialized with host=<inverse proxy endpoint>.\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Set to 55 mins to provide some safe margin.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m _GCP_ACCESS_TOKEN_TIMEOUT \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mtimedelta(minutes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m55\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/Caskroom/miniforge/base/envs/mlops/lib/python3.11/site-packages/kfp/_auth.py:24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moauth2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcredentials\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moauth2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice_account\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests_toolbelt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madapters\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mappengine\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwebbrowser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m open_new_tab\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'requests_toolbelt.adapters.appengine'"
     ]
    }
   ],
   "source": [
    "#titanic_kfp_pipeline.ipynb\n",
    "#Copyright 2020 Google LLC.\n",
    "#This software is provided as-is, without warranty or representation for any use or purpose.\n",
    "#Your use of it is subject to your agreements with Google.\n",
    "#Author: whjang@google.com\n",
    "#!pip3 install -U kfp\n",
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp.components import func_to_container_op\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "PIPELINE_HOST = \"7f292c1c27803874-dot-us-central1.pipelines.googleusercontent.com\"\n",
    "WORK_BUCKET = \"gs://practice-tensorflow\"\n",
    "EXPERIMENT_NAME = \"Titanic Draft Experiment\"\n",
    "\n",
    "# Function for determine deployment\n",
    "@func_to_container_op\n",
    "def check_and_deploy_op(ACC_CSV_GCS_URI) -> str:\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    import pandas as pd\n",
    "    acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    "    return acc_df[\"deploy\"].item()\n",
    "\n",
    "@func_to_container_op\n",
    "def finish_deploy_op(ACC_CSV_GCS_URI):\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'gcsfs'])\n",
    "    import pandas as pd\n",
    "    acc_df = pd.read_csv(ACC_CSV_GCS_URI)\n",
    "    acc_df[\"deploy\"] = \"done\"\n",
    "    acc_df.to_csv(ACC_CSV_GCS_URI)\n",
    "    print(\"Successfully new model was deployed\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"titanic-kubeflow-pipeline-demo\",\n",
    "    description=\"Titanic Kubeflow Pipeline demo embrassing AI Platform in Google Cloud\"\n",
    ")\n",
    "def titanic_pipeline(\n",
    "        PROJECT_ID,\n",
    "        WORK_BUCKET,\n",
    "        RAW_CSV_GCS_URI,\n",
    "        PREPROC_CSV_GCS_URI,\n",
    "        ACC_CSV_GCS_URI,\n",
    "        MODEL_PKL_GCS_URI,\n",
    "        MIN_ACC_PROGRESS,\n",
    "        STAGE_GCS_FOLDER,\n",
    "        TRAIN_ON_CLOUD,\n",
    "        AIPJOB_TRAINER_GCS_PATH,\n",
    "        AIPJOB_OUTPUT_GCS_PATH\n",
    "):\n",
    "    IMAGE_PREFIX = \"whjang-titanic\"\n",
    "    PREPROC_DIR = \"preprocess\"\n",
    "    TRAIN_DIR = \"train\"\n",
    "    MODEL_DIR = \"model\"\n",
    "\n",
    "    preprocess = dsl.ContainerOp(\n",
    "        name = \"Preprocess raw data and generate new one\",\n",
    "        image = f\"gcr.io/{str(PROJECT_ID)}/{IMAGE_PREFIX}-{PREPROC_DIR}:latest\",\n",
    "        arguments=[\n",
    "            \"--raw_csv_gcs_uri\", RAW_CSV_GCS_URI,\n",
    "            \"--preproc_csv_gcs_uri\", PREPROC_CSV_GCS_URI\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_args = [\n",
    "        \"--preproc_csv_gcs_uri\", str(PREPROC_CSV_GCS_URI),\n",
    "        \"--model_pkl_gcs_uri\", str(MODEL_PKL_GCS_URI),\n",
    "        \"--acc_csv_gcs_uri\", str(ACC_CSV_GCS_URI),\n",
    "        \"--min_acc_progress\", str(MIN_ACC_PROGRESS)\n",
    "    ]\n",
    "\n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == False) as check_condition1:\n",
    "        train = dsl.ContainerOp(\n",
    "            name=\"Train\",\n",
    "            image = f\"gcr.io/{str(PROJECT_ID)}/{IMAGE_PREFIX}-{TRAIN_DIR}:latest\",\n",
    "            arguments = train_args,\n",
    "            file_outputs={\n",
    "                \"mlpipeline-metrics\": \"/mlpipeline-metrics.json\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "    with dsl.Condition(TRAIN_ON_CLOUD == True) as check_condition2:\n",
    "        aip_job_train_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/train/component.yaml\")\n",
    "        train = dsl.ContainerOp(\n",
    "            name=\"Train\",\n",
    "            image = f\"gcr.io/{str(PROJECT_ID)}/{IMAGE_PREFIX}-{TRAIN_DIR}:latest\",\n",
    "            arguments = train_args,\n",
    "            file_outputs={\n",
    "                \"mlpipeline-metrics\": \"/mlpipeline-metrics.json\"\n",
    "            }\n",
    "        )\n",
    "        help(aip_job_train_op)\n",
    "        aip_train = aip_job_train_op(\n",
    "            project_id=PROJECT_ID,\n",
    "            python_module=\"train.titanic_train\",\n",
    "            package_uris=json.dumps([str(AIPJOB_TRAINER_GCS_PATH)]),\n",
    "            region=\"us-central1\",\n",
    "            args=json.dumps(train_args),\n",
    "            job_dir=AIPJOB_OUTPUT_GCS_PATH,\n",
    "            python_version=\"3.7\",\n",
    "            runtime_version=\"1.15\", #cf. 2.1\n",
    "            master_image_uri=\"\",\n",
    "            worker_image_uri=\"\",\n",
    "            training_input=\"\",\n",
    "            job_id_prefix=\"\",\n",
    "            job_id=\"\",\n",
    "            wait_interval=5\n",
    "        )\n",
    "\n",
    "    check_deploy = check_and_deploy_op(ACC_CSV_GCS_URI)\n",
    "    with dsl.Condition(check_deploy.ouput == \"pending\"):\n",
    "        aip_model_deploy_op = comp.load_component_from_url(\"https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/deploy/component.yaml\")\n",
    "        help(aip_model_deploy_op)\n",
    "        aip_model_deploy = aip_model_deploy_op(\n",
    "            model_uri=str(WORK_BUCKET) + \"/\" + MODEL_DIR,\n",
    "            project_id=PROJECT_ID,\n",
    "            model_id=\"\",\n",
    "            version_id=\"\",\n",
    "            runtime_version=\"1.15\",  #cf. 2.1\n",
    "            python_version=\"3.7\",\n",
    "            version=\"\",\n",
    "            replace_existing_version=\"False\",\n",
    "            set_default=\"True\",\n",
    "            wait_interval=5\n",
    "        )\n",
    "    lastStep = finish_deploy_op(ACC_CSV_GCS_URI)\n",
    "\n",
    "    check_condition1.after(preprocess)\n",
    "    check_condition2.after(preprocess)\n",
    "    check_deploy.after(aip_train)\n",
    "    lastStep.after(aip_model_deploy)\n",
    "\n",
    "    train.execution_options.caching_strategy.max_cache_staleness = \"POD\"\n",
    "    aip_train.execution_options.caching_strategy.max_cache_staleness = \"POD\"\n",
    "    check_deploy.execution_options.caching_strategy.max_cache_staleness = \"POD\"\n",
    "    aip_model_deploy.execution_options.caching_strategy.max_cache_staleness = \"POD\"\n",
    "    lastStep.execution_options.caching_strategy.max_cache_staleness = \"POD\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"PROJECT_ID\": \"learned-iridium-385115\",\n",
    "    \"WORK_BUCKET\": WORK_BUCKET,\n",
    "    \"RAW_CSV_GCS_URI\": WORK_BUCKET + \"/rawdata/train.csv\",\n",
    "    \"PREPROC_CSV_GCS_URI\": WORK_BUCKET + \"/preprocdata/processed_train.csv\",\n",
    "    \"ACC_CSV_GCS_URI\": WORK_BUCKET + \"/latestacc/accuracy.csv\",\n",
    "    \"MODEL_PKL_GCS_URI\": WORK_BUCKET + \"/model/model.pkl\",\n",
    "    \"MIN_ACC_PROGRESS\": 0.000001,\n",
    "    \"STAGE_GCS_FOLDER\": WORK_BUCKET + \"/stage\",\n",
    "    \"TRAIN_ON_CLOUD\": False,\n",
    "    \"AIPJOB_TRAINER_GCS_PATH\": WORK_BUCKET + \"/train/titanic_train.tar.gz\",\n",
    "    \"AIPJOB_OUTPUT_GCS_PATH\": WORK_BUCKET + \"/train/output/\"\n",
    "}\n",
    "\n",
    "# Notebook에서 테스트 한 예제이기 때문에, 쉘에서 실행시키시려면 main 함수 형태로 변경하셔야 합니다.\n",
    "# Dockerize한 전처리 모듈 및 ML 학습, 검증 모듈과 달리 @func_to_container_op를 사용한 lightweights 방식의 함수들도 있습니다. check_and_deploy_op와 finish_deploy_op입니다. @func_to_container_op는 이렇게 함수를 자동으로 Dockerize 하기 때문에 별도의 Dockerfile등을 정의할 필요가 없는 장점이 있지만, 필요한 외부 패키지등은 함수 내부에서 subprocess를 통해 각각 설치해야 합니다.\n",
    "\n",
    "client = kfp.Client(host=PIPELINE_HOST)\n",
    "#pipeline_name = \"titanic_pipelines.zip\"\n",
    "#compiler.Compiler().compile(titanic_pipeline, pipeline_name)\n",
    "#try:\n",
    "# pipeline = client.upload_pipeline(pipeline_package_path=pipeline_name, pipeline_name=pipeline_name)\n",
    "# print(\"uploaded:\" + pipeline.id)\n",
    "#except:\n",
    "# print(\"already exist\")\n",
    "client.create_run_from_pipeline_func(\n",
    "    titanic_pipeline,\n",
    "    arguments=args,\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
